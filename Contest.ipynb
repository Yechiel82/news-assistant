{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EciG5XnfquQL",
        "outputId": "46c29f17-c8f7-4a9a-d1b2-dc98c2679bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.21)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.352)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.6)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (4.39)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.105.0)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.1.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.16.3)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (28.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.6)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.3)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.74)\n",
            "Requirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.2)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.11.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.26.18)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub)\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Installing collected packages: types-urllib3, types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.6 types-urllib3-1.26.25.14\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken chromadb langchain kaleido python-multipart cohere langchainhub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"https://medium.com/@johnidouglasmarangon/how-to-summarize-text-with-openai-and-langchain-e038fc922af\")\n",
        "docs = loader.load()\n",
        "OPENAI_API_KEY =\"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\"\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\", api_key = OPENAI_API_KEY)\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "chain.run(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "T7ra_hwjq47H",
        "outputId": "8496b17a-e5ea-4f1b-e14d-7fcdfbc29e52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The article provides a guide on how to use OpenAI models and LangChain framework to summarize text using Large Language Models (LLMs). It explains the concept of text summarization, the use of LLMs, setting up the environment, reading the documents to summarize, and the summarization process. It also emphasizes the importance of human evaluation in gauging summarization tasks and provides a preview of upcoming posts on using LLama 2 and fine-tuning LLM models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "api_key = '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'\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
        "\n",
        "# Map\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{docs}\n",
        "Based on this list of docs, please identify the main themes\n",
        "Helpful Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X1dBcfrVrA-X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "map_prompt = hub.pull(\"rlm/map-prompt\")\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "metadata": {
        "id": "_S1YkXPLri1U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary of the main themes.\n",
        "Helpful Answer:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)"
      ],
      "metadata": {
        "id": "HhIfKyqprmHm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note we can also get this from the prompt hub, as noted above\n",
        "reduce_prompt = hub.pull(\"rlm/map-prompt\")\n",
        "reduce_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzvbem36tL4t",
        "outputId": "073f7e0c-9835-434e-a3f9-05dbdd15d1f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")\n",
        "\n",
        "# Combines and iteravely reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=4000,\n",
        ")"
      ],
      "metadata": {
        "id": "dJBJsnAktckk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0\n",
        ")\n",
        "split_docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "BLa7OAZ-tfVd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(map_reduce_chain.run(split_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX6ja_4QtjQw",
        "outputId": "b59804cb-a9d3-44bb-afd3-1256e77e6297"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main themes identified in the list of documents are:\n",
            "\n",
            "1. Summarization using OpenAI and LangChain\n",
            "2. Introduction to LangChain framework\n",
            "3. Understanding the task of text summarization\n",
            "4. Explanation of Large Language Models (LLM)\n",
            "5. Setting up the environment for text summarization\n",
            "6. Reading and splitting the documents to summarize\n",
            "7. Summarizing the text using OpenAI API\n",
            "8. Prompt engineering for guiding the LLM\n",
            "9. Computing the number of tokens and dealing with token limit\n",
            "10. Implementing text summarization using LangChain\n",
            "11. Importance of human evaluation in summarization tasks\n",
            "12. Scalable summarization solution using OpenAI and LangChain\n",
            "13. Future developments in language models and upcoming posts on LLama 2 and fine-tuning LLM models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REFINE"
      ],
      "metadata": {
        "id": "EmiO5F05t4UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
        "chain.run(split_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "eieke0VTt5lO",
        "outputId": "38ac6d32-f994-46f9-da02-138ed7de42c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The article discusses how to summarize text using OpenAI models and the LangChain framework. It explains the two main approaches to text summarization: extractive summarization and abstractive summarization. The article also provides an overview of Large Language Models (LLMs) and their applications in natural language processing tasks. It then guides the reader through the process of setting up the environment, reading the documents to summarize, and actually summarizing the text using the OpenAI API. The article emphasizes the importance of prompt engineering and human evaluation in achieving accurate and concise summaries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are news producer please create texts for news dictor actor from this texts\n",
        "\n",
        "{text}\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "# prompt_template = \"\"\"Write a concise summary of the following:\n",
        "# {text}\n",
        "# CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "refine_template = (\n",
        "    \"Your task is to generate a final summary for the news anchor.\\n\"\n",
        "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing summary\"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{text}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original summary in Italian\"\n",
        "    \"If the context isn't useful, return the original summary.\"\n",
        ")\n",
        "refine_prompt = PromptTemplate.from_template(refine_template)\n",
        "chain = load_summarize_chain(\n",
        "    llm=llm,\n",
        "    chain_type=\"refine\",\n",
        "    question_prompt=prompt,\n",
        "    refine_prompt=refine_prompt,\n",
        "    return_intermediate_steps=True,\n",
        "    input_key=\"input_documents\",\n",
        "    output_key=\"output_text\",\n",
        ")\n",
        "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
      ],
      "metadata": {
        "id": "XAhDjiESt8yz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Gk7p_puAll",
        "outputId": "3dee1168-ee08-4055-c25a-b589d28a04b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this article, the author discusses the process of text summarization using OpenAI models and the LangChain framework. They explain the two main approaches to text summarization: extractive summarization and abstractive summarization. The author also introduces Large Language Models (LLMs), which are advanced AI models that can understand and generate natural language. They provide an overview of setting up the environment and reading the documents to summarize. The article then explains how to use the OpenAI API and LangChain to summarize the text, including prompt engineering and counting the number of tokens. The author concludes by emphasizing the importance of human evaluation in gauging the quality of the summarization and hints at future topics they will cover in upcoming articles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yqKlJ1_uDXT",
        "outputId": "05d60793-29ae-43ca-98eb-1696d28c1f19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this article, the author discusses the process of text summarization using OpenAI models and the LangChain framework. They explain the two main approaches to text summarization: extractive summarization and abstractive summarization. The author also introduces Large Language Models (LLMs), which are advanced AI models that can understand and generate natural language. They provide an overview of setting up the environment and reading the documents to summarize. The article then explains how to use the OpenAI API and LangChain to summarize the text, including prompt engineering and counting the number of tokens. The author concludes by emphasizing the importance of human evaluation in gauging the quality of the summarization and hints at future topics they will cover in upcoming articles.\n"
          ]
        }
      ]
    }
  ]
}